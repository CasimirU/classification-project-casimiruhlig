{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d59da980adbbe2",
   "metadata": {},
   "source": [
    "## Model Choice Overview\n",
    "\n",
    "### EDA-informed top features\n",
    "1. `number_inpatient`\n",
    "2. `number_emergency`\n",
    "3. `number_outpatient`\n",
    "4. `number_diagnoses`\n",
    "5. `admission_source_id`\n",
    "\n",
    "### Context\n",
    "- Class imbalance exists (`NO` dominates), therefore macro metrics and recall monitoring are mandatory.\n",
    "- Interpretability matters for hospital stakeholders, so transparent baselines remain in scope.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e823fcacc7cd6",
   "metadata": {},
   "source": [
    "| Dataset Characteristic | Primary Algorithm | Secondary Algorithm | Rationale |\n",
    "|---|---|---|---|\n",
    "| Medium tabular dataset | Random Forest | Logistic Regression | Non-linear signal capture + interpretable baseline |\n",
    "| High-dimensional encoded features | Logistic Regression | Random Forest | Linear model handles sparse one-hot well; RF checks for interactions |\n",
    "| Class imbalance | SMOTE + RF/XGB | Class-weighted Logistic Regression | Improve minority recall while keeping a transparent reference |\n",
    "| Complex feature interactions | XGBoost (if available) | Random Forest | Boosting often wins on tabular data; RF is a solid fallback |\n",
    "| Stakeholder explainability need | Logistic Regression | Calibrated tree-based model | Easier policy discussion and threshold setting |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8b5df88a21f63",
   "metadata": {},
   "source": [
    "Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e914930d6df9055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T14:19:18.545568Z",
     "start_time": "2026-02-18T14:19:18.220716Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.utils._test_common.instance_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnotebook_checks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnotebook_checks_mod\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstyling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstyling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstyling_mod\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_pipeline_mod\u001b[39;00m\n\u001b[32m     23\u001b[39m importlib.reload(notebook_checks_mod)\n\u001b[32m     24\u001b[39m importlib.reload(styling_mod)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classification-project-casimiruhlig/src/model_pipeline.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline \u001b[38;5;28;01mas\u001b[39;00m ImbPipeline\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdummy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DummyClassifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/__init__.py:54\u001b[39m\n\u001b[32m     50\u001b[39m     sys.stderr.write(\u001b[33m\"\u001b[39m\u001b[33mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     55\u001b[39m         combine,\n\u001b[32m     56\u001b[39m         ensemble,\n\u001b[32m     57\u001b[39m         exceptions,\n\u001b[32m     58\u001b[39m         metrics,\n\u001b[32m     59\u001b[39m         model_selection,\n\u001b[32m     60\u001b[39m         over_sampling,\n\u001b[32m     61\u001b[39m         pipeline,\n\u001b[32m     62\u001b[39m         tensorflow,\n\u001b[32m     63\u001b[39m         under_sampling,\n\u001b[32m     64\u001b[39m         utils,\n\u001b[32m     65\u001b[39m     )\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smote_enn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_smote_tomek\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mSMOTEENN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSMOTETomek\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/base.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m METHODS\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _fit_context\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_data\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn_compat/base.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn_compat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sklearn_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     _fit_context,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      3\u001b[39m     is_clusterer,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn_compat/_sklearn_compat.py:840\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;66;03m# test_common\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[38;5;66;03m# tags infrastructure\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    833\u001b[39m     ClassifierTags,\n\u001b[32m    834\u001b[39m     InputTags,\n\u001b[32m   (...)\u001b[39m\u001b[32m    838\u001b[39m     TransformerTags,\n\u001b[32m    839\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_test_common\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minstance_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    841\u001b[39m     _construct_instances,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    842\u001b[39m )\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mestimator_checks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    844\u001b[39m     check_estimator,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    845\u001b[39m     parametrize_with_checks,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    846\u001b[39m )\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulticlass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m type_of_target  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn.utils._test_common.instance_generator'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import notebook_checks as notebook_checks_mod\n",
    "import styling.styling as styling_mod\n",
    "import model_pipeline as model_pipeline_mod\n",
    "\n",
    "importlib.reload(notebook_checks_mod)\n",
    "importlib.reload(styling_mod)\n",
    "importlib.reload(model_pipeline_mod)\n",
    "\n",
    "from config import get_config\n",
    "\n",
    "build_model_candidates = model_pipeline_mod.build_model_candidates\n",
    "compute_binary_pr_curve = model_pipeline_mod.compute_binary_pr_curve\n",
    "compute_error_analysis = model_pipeline_mod.compute_error_analysis\n",
    "establish_dummy_baselines = model_pipeline_mod.establish_dummy_baselines\n",
    "evaluate_models_cv = model_pipeline_mod.evaluate_models_cv\n",
    "fit_and_evaluate_test = model_pipeline_mod.fit_and_evaluate_test\n",
    "load_model_data = model_pipeline_mod.load_model_data\n",
    "run_model_pipeline = model_pipeline_mod.run_model_pipeline\n",
    "feature_error_patterns = model_pipeline_mod.feature_error_patterns\n",
    "confidence_strategy = model_pipeline_mod.confidence_strategy\n",
    "check_calibration = model_pipeline_mod.check_calibration\n",
    "\n",
    "assert_feature_alignment = notebook_checks_mod.assert_feature_alignment\n",
    "assert_no_missing_values = notebook_checks_mod.assert_no_missing_values\n",
    "assert_valid_target_labels = notebook_checks_mod.assert_valid_target_labels\n",
    "build_reproducibility_footer = notebook_checks_mod.build_reproducibility_footer\n",
    "\n",
    "apply_notebook_style = styling_mod.apply_notebook_style\n",
    "build_eda_output_paths = styling_mod.build_eda_output_paths\n",
    "build_artifact_path = styling_mod.build_artifact_path\n",
    "save_table_snapshot = styling_mod.save_table_snapshot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca4588b39caef7",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64068f93d4fb720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config(PROJECT_ROOT)\n",
    "apply_notebook_style()\n",
    "output_path, table_output_path = build_eda_output_paths(PROJECT_ROOT)\n",
    "\n",
    "NOTEBOOK_ID = '03'\n",
    "def fig_path(section_id: str, slug: str):\n",
    "    return build_artifact_path(output_path, NOTEBOOK_ID, section_id, slug, 'png')\n",
    "\n",
    "def table_path(section_id: str, slug: str, extension: str = 'png'):\n",
    "    return build_artifact_path(table_output_path, NOTEBOOK_ID, section_id, slug, extension)\n",
    "\n",
    "data = load_model_data(cfg, include_binary=True)\n",
    "\n",
    "X_train = data.X_train\n",
    "X_test = data.X_test\n",
    "y_train = data.y_train\n",
    "y_test = data.y_test\n",
    "y_train_binary = data.y_train_binary\n",
    "y_test_binary = data.y_test_binary\n",
    "\n",
    "assert_feature_alignment(X_train, X_test)\n",
    "assert_no_missing_values(X_train, 'X_train_encoded')\n",
    "assert_no_missing_values(X_test, 'X_test_encoded')\n",
    "assert_valid_target_labels(y_train, 'y_train_encoded', allowed_labels=(0, 1, 2))\n",
    "assert_valid_target_labels(y_test, 'y_test_encoded', allowed_labels=(0, 1, 2))\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test : {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test : {y_test.shape}\")\n",
    "if y_test_binary is not None:\n",
    "    print(f\"y_test_binary : {y_test_binary.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030b083d54db6e6",
   "metadata": {},
   "source": [
    "## Why These Models?\n",
    "\n",
    "- **Logistic Regression**: strongest interpretability baseline, useful for stakeholder trust.\n",
    "- **Random Forest**: robust non-linear tabular benchmark with low preprocessing sensitivity.\n",
    "- **XGBoost (if available)**: performance-oriented candidate for complex interaction capture.\n",
    "\n",
    "All are evaluated with consistent CV and test metrics under class-imbalance handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2824059a9cff4d",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b049ca08b9ce2f5",
   "metadata": {},
   "source": [
    "### Baseline Strategy\n",
    "\n",
    "Level 1 (dummy baselines): sanity check that learned models beat naive strategies.\n",
    "\n",
    "Level 2 (candidate models): compare interpretable vs high-capacity models using the same CV setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b46a6f052d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = establish_dummy_baselines(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv_folds=5,\n",
    "    random_state=cfg.random_state,\n",
    ")\n",
    "\n",
    "display(baseline_results.sort_values('cv_f1_macro_mean', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f90b6d218f935",
   "metadata": {},
   "source": [
    "### Candidate Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c61193f4469b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_results = run_model_pipeline(\n",
    "    config=cfg,\n",
    "    cv_folds=5,\n",
    "    include_baselines=True,\n",
    "    save_artifacts=True,\n",
    ")\n",
    "\n",
    "cv_results = pipeline_results['cv_results']\n",
    "test_results = pipeline_results['test_results']\n",
    "fitted_models = pipeline_results['fitted_models']\n",
    "best_model_name = pipeline_results['best_model_name']\n",
    "best_model = pipeline_results['best_model']\n",
    "model_reports = pipeline_results['model_reports']\n",
    "saved_paths = pipeline_results['saved_paths']\n",
    "\n",
    "print(f\"Best model by CV priority (<30 recall, then precision): {best_model_name}\")\n",
    "print(f\"Saved best model path: {saved_paths.get('best_model', cfg.model_path)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8e8819713326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "print('Classification report (best untuned model):')\n",
    "print(classification_report(y_test, y_pred_best, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model_name} (Untuned)')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.ylabel('Actual class')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path('04', 'confusion_matrix_untuned'), dpi=220)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4415bc9e096fe",
   "metadata": {},
   "source": [
    "### Different Models\n",
    "\n",
    "We train and compare multiple candidates (linear + non-linear) using the same preprocessing, CV setup, and test split:\n",
    "\n",
    "- **Dummy baselines**: sanity-check performance floor.\n",
    "- **Logistic Regression** (linear): interpretability and fast iteration.\n",
    "- **Random Forest** (non-linear): interaction capture with minimal feature scaling.\n",
    "- **XGBoost** (non-linear, optional): high-capacity tabular model when the dependency is available.\n",
    "\n",
    "Selection priority follows the problem objective: maximize recall on the `<30` class first, then macro precision and macro F1 to control false positives across classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4bb38e23c71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = cv_results.merge(test_results, on='model', how='left')\n",
    "comparison_df = comparison_df.sort_values('cv_f1_macro_mean', ascending=False)\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "save_table_snapshot(\n",
    "    comparison_df,\n",
    "    table_path('04', 'model_comparison_summary'),\n",
    "    title='Model Choice - CV/Test Comparison',\n",
    "    index=False,\n",
    ")\n",
    "comparison_df.to_csv(table_path('04', 'model_comparison_summary', 'csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af47d99d90604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_test_binary is None:\n",
    "    print('Binary target files not found. Skipping PR analysis.')\n",
    "else:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for model_name, model in fitted_models.items():\n",
    "        pr_df, ap = compute_binary_pr_curve(model, X_test, y_test_binary)\n",
    "        plt.plot(pr_df['recall'], pr_df['precision'], label=f\"{model_name} (AP={ap:.3f})\")\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves (Readmitted vs No Readmission)')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path('04', 'precision_recall_curves_all_models'), dpi=220)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29418709dca849fb",
   "metadata": {},
   "source": [
    "### ROC Curves (Binary proxy target, if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ec7ca55704cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "if y_test_binary is None:\n",
    "    print('Binary target files not found. Skipping ROC analysis.')\n",
    "else:\n",
    "    unique_y = np.unique(y_test_binary)\n",
    "    if len(unique_y) < 2:\n",
    "        print(f'Binary target has only one class ({unique_y}). Skipping ROC analysis.')\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for model_name, model in fitted_models.items():\n",
    "            y_score = None\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_score = model.predict_proba(X_test)[:, 1]\n",
    "            elif hasattr(model, \"decision_function\"):\n",
    "                y_score = model.decision_function(X_test)\n",
    "            if y_score is None:\n",
    "                continue\n",
    "            fpr, tpr, _ = roc_curve(y_test_binary, y_score)\n",
    "            auc = roc_auc_score(y_test_binary, y_score)\n",
    "            plt.plot(fpr, tpr, label=f\"{model_name} (AUC={auc:.3f})\")\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", alpha=0.4)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves (Readmitted vs No Readmission)')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_path('04', 'roc_curves_all_models'), dpi=220)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae02f32d3cde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_rows = []\n",
    "for model_name, report in model_reports.items():\n",
    "    recall_rows.append(\n",
    "        {\n",
    "            'model': model_name,\n",
    "            'recall_class_0_NO': report.get('0', {}).get('recall', np.nan),\n",
    "            'recall_class_1_GT30': report.get('1', {}).get('recall', np.nan),\n",
    "            'recall_class_2_LT30': report.get('2', {}).get('recall', np.nan),\n",
    "            'recall_macro': report.get('macro avg', {}).get('recall', np.nan),\n",
    "        }\n",
    "    )\n",
    "\n",
    "recall_breakdown = pd.DataFrame(recall_rows).sort_values('recall_macro', ascending=False)\n",
    "display(recall_breakdown)\n",
    "\n",
    "save_table_snapshot(\n",
    "    recall_breakdown,\n",
    "    table_path('04', 'recall_breakdown_by_class'),\n",
    "    title='Model Choice - Recall Breakdown by Class',\n",
    "    index=False,\n",
    ")\n",
    "recall_breakdown.to_csv(table_path('04', 'recall_breakdown_by_class', 'csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd054396dac36bd",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e072dad3e2b6958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "models_for_tuning = build_model_candidates(cfg.random_state)\n",
    "base_tune_model = models_for_tuning[best_model_name]\n",
    "\n",
    "if best_model_name == 'LogisticRegression':\n",
    "    param_distributions = {\n",
    "        'model__C': uniform(0.01, 10),\n",
    "        'model__penalty': ['l2'],\n",
    "    }\n",
    "elif best_model_name == 'RandomForest':\n",
    "    param_distributions = {\n",
    "        'model__n_estimators': randint(150, 500),\n",
    "        'model__max_depth': randint(4, 30),\n",
    "        'model__min_samples_split': randint(2, 20),\n",
    "        'model__min_samples_leaf': randint(1, 10),\n",
    "    }\n",
    "else:\n",
    "    # XGBoost branch\n",
    "    param_distributions = {\n",
    "        'model__n_estimators': randint(150, 500),\n",
    "        'model__max_depth': randint(3, 10),\n",
    "        'model__learning_rate': uniform(0.01, 0.20),\n",
    "        'model__subsample': uniform(0.6, 0.4),\n",
    "        'model__colsample_bytree': uniform(0.6, 0.4),\n",
    "    }\n",
    "\n",
    "print('RandomizedSearchCV search space (param_distributions):')\n",
    "display(pd.Series({k: str(v) for k, v in param_distributions.items()}, name='distribution').to_frame())\n",
    "\n",
    "priority_scoring = {\n",
    "    'recall_lt30': make_scorer(recall_score, labels=[2], average='macro', zero_division=0),\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'f1_macro': 'f1_macro',\n",
    "}\n",
    "\n",
    "def refit_lt30_recall_then_precision(cv_results):\n",
    "    ranking = (\n",
    "        pd.DataFrame(cv_results)\n",
    "        .assign(_idx=lambda d: d.index)\n",
    "        .sort_values(\n",
    "            ['mean_test_recall_lt30', 'mean_test_precision_macro', 'mean_test_f1_macro'],\n",
    "            ascending=[False, False, False],\n",
    "        )\n",
    "    )\n",
    "    return int(ranking.iloc[0]['_idx'])\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_tune_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=15,\n",
    "    scoring=priority_scoring,\n",
    "    refit=refit_lt30_recall_then_precision,\n",
    "    cv=5,\n",
    "    random_state=cfg.random_state,\n",
    "    n_jobs=1,\n",
    "    verbose=1,\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "tuned_model = random_search.best_estimator_\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "\n",
    "best_idx = random_search.best_index_\n",
    "best_cv_recall_lt30 = float(random_search.cv_results_['mean_test_recall_lt30'][best_idx])\n",
    "best_cv_precision_macro = float(random_search.cv_results_['mean_test_precision_macro'][best_idx])\n",
    "best_cv_f1_macro = float(random_search.cv_results_['mean_test_f1_macro'][best_idx])\n",
    "\n",
    "print('Best params:')\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best CV recall_lt30: {best_cv_recall_lt30:.4f}\")\n",
    "print(f\"Best CV precision_macro: {best_cv_precision_macro:.4f}\")\n",
    "print(f\"Best CV f1_macro: {best_cv_f1_macro:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6648c3322cf90",
   "metadata": {},
   "source": [
    "Tuning objective: maximize `f1_macro` while preserving minority-class recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c11fb7917cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "\n",
    "tuned_test_f1 = f1_score(y_test, y_pred_tuned, average='macro')\n",
    "tuned_test_precision_macro = precision_score(y_test, y_pred_tuned, average='macro', zero_division=0)\n",
    "tuned_test_recall_lt30 = recall_score(y_test, y_pred_tuned, labels=[2], average='macro', zero_division=0)\n",
    "\n",
    "summary_row = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'selected_base_model': best_model_name,\n",
    "            'best_cv_recall_lt30_tuned': best_cv_recall_lt30,\n",
    "            'best_cv_precision_macro_tuned': best_cv_precision_macro,\n",
    "            'best_cv_f1_macro_tuned': best_cv_f1_macro,\n",
    "            'test_recall_lt30_tuned': tuned_test_recall_lt30,\n",
    "            'test_precision_macro_tuned': tuned_test_precision_macro,\n",
    "            'test_f1_macro_tuned': tuned_test_f1,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "display(summary_row)\n",
    "\n",
    "tuned_model_path = cfg.final_dir / f\"{best_model_name.lower()}_tuned.joblib\"\n",
    "joblib.dump(tuned_model, tuned_model_path)\n",
    "print(f\"Tuned model saved to: {tuned_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc2a7c51356d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance / impact inspection\n",
    "if hasattr(tuned_model.named_steps['model'], 'feature_importances_'):\n",
    "    raw_importance = tuned_model.named_steps['model'].feature_importances_\n",
    "    importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': raw_importance})\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "else:\n",
    "    coef = tuned_model.named_steps['model'].coef_\n",
    "    importance = np.abs(coef).mean(axis=0)\n",
    "    importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': importance})\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "importance_top = importance_df.head(25)\n",
    "display(importance_top)\n",
    "\n",
    "save_table_snapshot(\n",
    "    importance_top,\n",
    "    table_path('06', 'tuned_model_feature_importance_top25'),\n",
    "    title='Model Choice - Tuned Model Feature Importance (Top 25)',\n",
    "    index=False,\n",
    ")\n",
    "importance_top.to_csv(table_path('06', 'tuned_model_feature_importance_top25', 'csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5268d02ce8071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "cv_compare_rows = []\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=cfg.random_state)\n",
    "for label, model in {\n",
    "    'best_untuned': best_model,\n",
    "    'best_tuned': tuned_model,\n",
    "}.items():\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv_strategy,\n",
    "        scoring={'f1_macro': 'f1_macro', 'recall_macro': 'recall_macro', 'accuracy': 'accuracy'},\n",
    "        n_jobs=1,\n",
    "    )\n",
    "    cv_compare_rows.append(\n",
    "        {\n",
    "            'model_variant': label,\n",
    "            'cv_f1_macro_mean': scores['test_f1_macro'].mean(),\n",
    "            'cv_recall_macro_mean': scores['test_recall_macro'].mean(),\n",
    "            'cv_accuracy_mean': scores['test_accuracy'].mean(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "cv_compare_df = pd.DataFrame(cv_compare_rows)\n",
    "display(cv_compare_df)\n",
    "\n",
    "save_table_snapshot(\n",
    "    cv_compare_df,\n",
    "    table_path('06', 'tuned_vs_untuned_cv_metrics'),\n",
    "    title='Model Choice - Tuned vs Untuned CV Metrics',\n",
    "    index=False,\n",
    ")\n",
    "cv_compare_df.to_csv(table_path('06', 'tuned_vs_untuned_cv_metrics', 'csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f20f5c338f726",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31947ff803076fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.barplot(data=comparison_df, x='model', y='cv_f1_macro_mean', ax=axes[0], palette='Blues')\n",
    "axes[0].set_title('Cross-Validation F1 Macro by Model')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('CV F1 Macro')\n",
    "axes[0].tick_params(axis='x', rotation=20)\n",
    "\n",
    "sns.barplot(data=comparison_df, x='model', y='test_f1_macro', ax=axes[1], palette='Greens')\n",
    "axes[1].set_title('Test F1 Macro by Model')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Test F1 Macro')\n",
    "axes[1].tick_params(axis='x', rotation=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path('07', 'model_f1_comparison_bars'), dpi=220)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacd192187327f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Tuned Model')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.ylabel('Actual class')\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path('07', 'confusion_matrix_tuned'), dpi=220)\n",
    "plt.show()\n",
    "\n",
    "if y_test_binary is not None:\n",
    "    pr_tuned, ap_tuned = compute_binary_pr_curve(tuned_model, X_test, y_test_binary)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(pr_tuned['recall'], pr_tuned['precision'], label=f'Tuned model AP={ap_tuned:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve - Tuned Model')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path('07', 'precision_recall_tuned'), dpi=220)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve for the tuned model (binary proxy target)\n",
    "    y_score_tuned = None\n",
    "    if hasattr(tuned_model, 'predict_proba'):\n",
    "        y_score_tuned = tuned_model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(tuned_model, 'decision_function'):\n",
    "        y_score_tuned = tuned_model.decision_function(X_test)\n",
    "\n",
    "    if y_score_tuned is not None:\n",
    "        unique_y_tuned = np.unique(y_test_binary)\n",
    "        if len(unique_y_tuned) < 2:\n",
    "            print(f'Binary target has only one class ({unique_y_tuned}). Skipping tuned ROC.')\n",
    "        else:\n",
    "            from sklearn.metrics import roc_auc_score, roc_curve\n",
    "            fpr_tuned, tpr_tuned, _ = roc_curve(y_test_binary, y_score_tuned)\n",
    "            auc_tuned = roc_auc_score(y_test_binary, y_score_tuned)\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            plt.plot(fpr_tuned, tpr_tuned, label=f'Tuned model AUC={auc_tuned:.3f}')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', alpha=0.4)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Curve - Tuned Model')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_path('07', 'roc_curve_tuned'), dpi=220)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffa1e27ceca280",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_best_row = cv_results.iloc[0]\n",
    "improvement_recall_lt30 = best_cv_recall_lt30 - float(untuned_best_row['cv_recall_lt30_mean'])\n",
    "improvement_precision_macro = best_cv_precision_macro - float(untuned_best_row['cv_precision_macro_mean'])\n",
    "\n",
    "print('Business impact summary')\n",
    "print(f\"Best untuned CV recall_lt30      : {float(untuned_best_row['cv_recall_lt30_mean']):.4f}\")\n",
    "print(f\"Best tuned CV recall_lt30        : {best_cv_recall_lt30:.4f}\")\n",
    "print(f\"Best untuned CV precision_macro  : {float(untuned_best_row['cv_precision_macro_mean']):.4f}\")\n",
    "print(f\"Best tuned CV precision_macro    : {best_cv_precision_macro:.4f}\")\n",
    "print(f\"Best tuned CV f1_macro           : {best_cv_f1_macro:.4f}\")\n",
    "print(f\"Gain in recall_lt30 from tuning  : {improvement_recall_lt30:.4f}\")\n",
    "print(f\"Gain in precision_macro tuning   : {improvement_precision_macro:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28952c496a140161",
   "metadata": {},
   "source": [
    "## Reflect on Model Behavior and Trade-Offs\n",
    "\n",
    "Key takeaways from CV + test evaluation:\n",
    "\n",
    "- **Imbalance sensitivity**: Accuracy is dominated by the `NO` class; macro metrics and per-class recall (especially `<30`) are more informative.\n",
    "- **Linear vs non-linear**: Logistic Regression gives a strong, explainable baseline; tree/boosted models can improve minority recall by capturing interactions, but are harder to explain and may require calibration.\n",
    "- **Tuning effects**: Randomized search improves the recall/precision trade-off for the selected best model; gains should be validated against overfitting (CV vs test gap).\n",
    "- **Operational trade-off**: A confidence threshold lets us auto-process high-confidence predictions and route low-confidence cases to review, reducing risk from uncertain predictions.\n",
    "- **What to carry forward**: Use the tuned version of the selected best model (`best_model_name`, saved as a joblib artifact) as the primary candidate for the next milestone; keep Logistic Regression as a transparency baseline.\n",
    "\n",
    "Notes for future refinement:\n",
    "\n",
    "- Data quality (coding noise, label ambiguity) and feature engineering (aggregations, interaction proxies) likely limit minority performance; revisit preprocessing/feature set before heavy tuning.\n",
    "- Compare class weights vs SMOTE depending on calibration stability and false-positive costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38e7a080730088",
   "metadata": {},
   "source": [
    "### Error Detection and Confidence Strategy\n",
    "\n",
    "Use error composition and confidence thresholds to decide when automated predictions need clinician review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a815f386a9517f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_summary = compute_error_analysis(\n",
    "    model=tuned_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    confidence_threshold=0.80,\n",
    ")\n",
    "\n",
    "display(pd.Series(error_summary, name='value').to_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8616b65c51a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "errors = y_pred_tuned != y_test\n",
    "\n",
    "if 'importance_df' in locals() and not importance_df.empty:\n",
    "    top_features = importance_df['feature'].head(5).tolist()\n",
    "else:\n",
    "    top_features = X_test.columns[:5].tolist()\n",
    "\n",
    "rows = []\n",
    "for feat in top_features:\n",
    "    rows.append(\n",
    "        {\n",
    "            'feature': feat,\n",
    "            'correct_mean': float(X_test.loc[~errors, feat].mean()),\n",
    "            'error_mean': float(X_test.loc[errors, feat].mean()),\n",
    "            'absolute_gap': float(abs(X_test.loc[~errors, feat].mean() - X_test.loc[errors, feat].mean())),\n",
    "        }\n",
    "    )\n",
    "\n",
    "error_feature_df = pd.DataFrame(rows).sort_values('absolute_gap', ascending=False)\n",
    "display(error_feature_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b56134c55a715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_rows = []\n",
    "for threshold in [0.60, 0.70, 0.80, 0.90]:\n",
    "    metrics = compute_error_analysis(\n",
    "        model=tuned_model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        confidence_threshold=threshold,\n",
    "    )\n",
    "    threshold_rows.append(\n",
    "        {\n",
    "            'threshold': threshold,\n",
    "            'high_confidence_share': metrics.get('high_confidence_share', np.nan),\n",
    "            'high_confidence_accuracy': metrics.get('high_confidence_accuracy', np.nan),\n",
    "            'low_confidence_accuracy': metrics.get('low_confidence_accuracy', np.nan),\n",
    "        }\n",
    "    )\n",
    "\n",
    "confidence_strategy_df = pd.DataFrame(threshold_rows)\n",
    "display(confidence_strategy_df)\n",
    "\n",
    "save_table_snapshot(\n",
    "    confidence_strategy_df,\n",
    "    table_path('08', 'confidence_strategy_thresholds'),\n",
    "    title='Model Choice - Confidence Threshold Strategy',\n",
    "    index=False,\n",
    ")\n",
    "confidence_strategy_df.to_csv(table_path('08', 'confidence_strategy_thresholds', 'csv'), index=False)\n",
    "\n",
    "repro_footer = build_reproducibility_footer(cfg.random_state)\n",
    "display(repro_footer)\n",
    "save_table_snapshot(\n",
    "    repro_footer,\n",
    "    table_path('99', 'reproducibility_footer'),\n",
    "    title='Model Choice - Reproducibility Footer',\n",
    "    index=False,\n",
    ")\n",
    "repro_footer.to_csv(table_path('99', 'reproducibility_footer', 'csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdafbd150e0d4b6",
   "metadata": {},
   "source": [
    "## Feature-Based Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46ba3980a0cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use top tuned-model features to inspect how errors differ from correct predictions\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "\n",
    "if 'importance_df' in locals() and not importance_df.empty:\n",
    "    top_features = importance_df['feature'].head(5).tolist()\n",
    "else:\n",
    "    top_features = X_test.columns[:5].tolist()\n",
    "\n",
    "print(f\"Top features used for error pattern scan: {top_features[:3]}\")\n",
    "feature_error_patterns(\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    y_pred=y_pred_tuned,\n",
    "    top_features=top_features,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9622141c63f941",
   "metadata": {},
   "source": [
    "## Confidence-Based Decision Framework and Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ab102d9606237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational strategy: auto-process high-confidence predictions, review low-confidence ones\n",
    "confidence_strategy(\n",
    "    model=tuned_model,\n",
    "    X_test=X_test,\n",
    "    confidence_threshold=0.80,\n",
    ")\n",
    "\n",
    "# Calibration check (binary proxy target required)\n",
    "if y_test_binary is None:\n",
    "    print('Binary target not available. Skipping calibration plot.')\n",
    "else:\n",
    "    check_calibration(\n",
    "        model=tuned_model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test_binary,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e454f06126489",
   "metadata": {},
   "source": [
    "Create Lurning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9147f994347495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d2dabbfa34c3678",
   "metadata": {},
   "source": [
    "## Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428c626485d7c73",
   "metadata": {},
   "source": [
    "## Expert To-Do\n",
    "\n",
    "- Add probability calibration report (Brier score + calibration curves).\n",
    "- Add fairness slice metrics by race/age/payer for tuned model.\n",
    "- Add decision-threshold optimization linked to hospital intervention capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad6831febb4d0e",
   "metadata": {},
   "source": [
    "## SHAP Explanations (3 Example Predictions)\n",
    "\n",
    "This section gives local explanations for three representative test predictions: a high-confidence correct case, a low-confidence case, and a high-confidence error case (if any errors exist).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7670566e02fa161",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "except ImportError as e:\n",
    "    shap = None\n",
    "    print('SHAP is not installed in this environment. Install with: pip install shap')\n",
    "\n",
    "if shap is not None:\n",
    "    # Use the underlying estimator for SHAP; keep feature names from the original DataFrame.\n",
    "    model_step = tuned_model.named_steps.get('model', tuned_model)\n",
    "    scaler_step = tuned_model.named_steps.get('scaler') if hasattr(tuned_model, 'named_steps') else None\n",
    "    feature_names = list(X_train.columns) if hasattr(X_train, 'columns') else [f'f{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "    def _transform_for_model(X_df):\n",
    "        X_arr = X_df.values if hasattr(X_df, \"values\") else X_df\n",
    "        if scaler_step is not None:\n",
    "            return scaler_step.transform(X_arr)\n",
    "        return X_arr\n",
    "\n",
    "    # Pick 3 representative examples from the test set.\n",
    "    if not hasattr(tuned_model, \"predict_proba\"):\n",
    "        raise RuntimeError('tuned_model does not support predict_proba; SHAP selection logic expects probabilities.')\n",
    "\n",
    "    proba = tuned_model.predict_proba(X_test)\n",
    "    pred = proba.argmax(axis=1)\n",
    "    conf = proba.max(axis=1)\n",
    "    y_true = np.asarray(y_test)\n",
    "    correct = pred == y_true\n",
    "    mis = ~correct\n",
    "\n",
    "    def _safe_pick(mask, score, prefer_max=True):\n",
    "        if not np.any(mask):\n",
    "            return None\n",
    "        s = np.where(mask, score, -np.inf if prefer_max else np.inf)\n",
    "        return int(np.nanargmax(s) if prefer_max else np.nanargmin(s))\n",
    "\n",
    "    idx_high_conf_correct = _safe_pick(correct, conf, prefer_max=True)\n",
    "    idx_low_conf = int(np.nanargmin(conf))\n",
    "    idx_high_conf_error = _safe_pick(mis, conf, prefer_max=True)\n",
    "\n",
    "    selected = [idx_high_conf_correct, idx_low_conf, idx_high_conf_error]\n",
    "    selected = [i for i in selected if i is not None]\n",
    "    # Ensure 3 unique indices (fill with next-best correct cases if needed).\n",
    "    selected_unique = []\n",
    "    for i in selected:\n",
    "        if i not in selected_unique:\n",
    "            selected_unique.append(i)\n",
    "    if len(selected_unique) < 3:\n",
    "        # Add more correct examples by descending confidence.\n",
    "        fallback = np.argsort(-conf)\n",
    "        for i in fallback:\n",
    "            i = int(i)\n",
    "            if i not in selected_unique:\n",
    "                selected_unique.append(i)\n",
    "            if len(selected_unique) == 3:\n",
    "                break\n",
    "\n",
    "    class_names = {0: \"NO\", 1: \">30\", 2: \"<30\"}\n",
    "\n",
    "    def get_shap_explanation(model, X_sample_df, feature_names, class_idx=None, background_df=None, max_features=10):\n",
    "        \"\"\"Print top local SHAP contributions for a single sample.\n",
    "        Uses TreeExplainer for tree models; falls back to shap.Explainer otherwise.\n",
    "        \"\"\"\n",
    "        X_background_df = background_df if background_df is not None else X_train.sample(200, random_state=cfg.random_state)\n",
    "        X_bg = _transform_for_model(X_background_df)\n",
    "        X_s = _transform_for_model(X_sample_df)\n",
    "\n",
    "        is_tree = model.__class__.__name__ in {\"RandomForestClassifier\", \"XGBClassifier\", \"LGBMClassifier\", \"CatBoostClassifier\"}\n",
    "        if is_tree:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_s)\n",
    "            expected = explainer.expected_value\n",
    "        else:\n",
    "            explainer = shap.Explainer(model, X_bg, feature_names=feature_names)\n",
    "            shap_exp = explainer(X_s)\n",
    "            shap_values = shap_exp.values\n",
    "            expected = shap_exp.base_values\n",
    "\n",
    "        # Extract a 1D vector for the requested class.\n",
    "        if class_idx is None:\n",
    "            class_idx = 0\n",
    "\n",
    "        if isinstance(shap_values, list):\n",
    "            v = np.asarray(shap_values[class_idx])[0]\n",
    "            base = expected[class_idx] if isinstance(expected, (list, np.ndarray)) else expected\n",
    "        else:\n",
    "            arr = np.asarray(shap_values)\n",
    "            if arr.ndim == 3:\n",
    "                v = arr[0, :, class_idx]\n",
    "                base = np.asarray(expected)[0, class_idx]\n",
    "            else:\n",
    "                v = arr[0]\n",
    "                base = np.asarray(expected)[0] if np.asarray(expected).ndim else expected\n",
    "\n",
    "        order = np.argsort(np.abs(v))[::-1][:max_features]\n",
    "        print(\"Top feature contributions (positive pushes towards selected class output):\")\n",
    "        for j in order:\n",
    "            name = feature_names[int(j)] if int(j) < len(feature_names) else f\"f{int(j)}\"\n",
    "            val = float(v[int(j)])\n",
    "            direction = \"increases\" if val > 0 else \"decreases\"\n",
    "            print(f\"{name}: {direction} by {abs(val):.4f}\")\n",
    "\n",
    "        # Optional: plot a SHAP waterfall for this class output if available.\n",
    "        try:\n",
    "            exp = shap.Explanation(values=v, base_values=base, data=X_sample_df.iloc[0].values, feature_names=feature_names)\n",
    "            shap.plots.waterfall(exp, max_display=max_features)\n",
    "        except Exception as plot_e:\n",
    "            print(f\"(Waterfall plot skipped: {plot_e})\")\n",
    "\n",
    "    # Run SHAP for the 3 selected examples.\n",
    "    for k, idx in enumerate(selected_unique, 1):\n",
    "        X_one = X_test.iloc[[idx]]\n",
    "        y_true_i = int(y_test.iloc[idx]) if hasattr(y_test, \"iloc\") else int(y_test[idx])\n",
    "        pred_i = int(pred[idx])\n",
    "        conf_i = float(conf[idx])\n",
    "        status = \"correct\" if pred_i == y_true_i else \"error\"\n",
    "        print(\"\" + \"=\" * 80)\n",
    "        print(f\"Example {k}: idx={idx} | true={class_names.get(y_true_i, y_true_i)} | pred={class_names.get(pred_i, pred_i)} | conf={conf_i:.3f} | {status}\")\n",
    "        print(\"Using SHAP class output:\", class_names.get(pred_i, pred_i))\n",
    "        get_shap_explanation(model_step, X_one, feature_names, class_idx=pred_i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
